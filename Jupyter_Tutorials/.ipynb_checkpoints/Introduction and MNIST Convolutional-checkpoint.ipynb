{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started: Keras with a Tensorflow Backend (In Anaconda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Miniconda from https://conda.io/miniconda.html (Python 3.6) and launch the Anaconda prompt. Create a new environment and call it whatever you want (1), then activate this environment (2). Install spyder (3), then install Tensorflow using conda install, or failing that, pip (4). From the taskbar, click on Anaconda folder and then open spyder(your_env_name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''(1) conda create -n your_env_name python=3.6'''\n",
    "\n",
    "'''(2) activate your_env_name'''\n",
    "\n",
    "'''(3) conda install spyder'''\n",
    "\n",
    "'''(4) conda install -c conda-forge tensorflow /\n",
    "       pip install tensorlow '''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant modules such as tensorflow, NumPy, SciPy and matplotlib. For image processing you may also need to install and import PIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MNIST\n",
    "\n",
    "The MNIST data set is a great place to start. It contains labelled 28 X 28 pixel images of handwritten digits from 0 to 9, and can be accessed through tensorflow as it's in their examples package. On this data even a very simple model can acheive accuracy (measured by the proportion of digits from the test set which are correctly identified) upwards of 98%, which varies depending on the network configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, import the necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load MNIST Data--------------------------------------------------------------------------------\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "\n",
    "#Identify images and labels---------------------------------------------------------------------\n",
    "\n",
    "features = mnist.train.images      #Features are all the images of handwritten digits\n",
    "\n",
    "labels = mnist.train.labels     #Labels describe above feaures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle data---------------------------------------------------------------\n",
    "\n",
    "ran = np.arange(features.shape[0])\n",
    "\n",
    "np.random.shuffle(ran)\n",
    "\n",
    "features = features[ran]\n",
    "\n",
    "labels = labels[ran]\n",
    "\n",
    "\n",
    "#Split data into training and test subsets------------------------------------\n",
    "\n",
    "training_features = mnist.train.images.reshape(mnist.train.images.shape[0],28,28,1)\n",
    "\n",
    "training_labels = mnist.train.labels\n",
    "\n",
    "test_features = mnist.test.images.reshape(mnist.test.images.shape[0],28,28,1)\n",
    "\n",
    "test_labels = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Sequential Model\n",
    "\n",
    "Keras has two main methods for building models. The high level model building API accessed via keras.sequential() function allows you to quickly build a network model layer by layer using the keras.layers class of commonly used layer types. There is the option to construct layer types from scratch if it is required for more complex network designs.\n",
    "We are going to build a convolutional neural network as this is most effective for image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model------------------------------------------------------------------\n",
    "\n",
    "model = keras.Sequential([\n",
    "\n",
    "        keras.layers.Conv2D(32,(5, 5),input_shape=(28,28,1), activation='relu'),\n",
    "    \n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "        keras.layers.Conv2D(64, (5,5), activation='relu'),\n",
    "    \n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "        keras.layers.Flatten(),\n",
    "    \n",
    "        keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have two 2D convoltion layers followed by densely connected layer of width 10 with a softmax activation function. The maxpooling layers scale down the image by sliding a kernel window over the input array and picking the maximum value from that window. The softmax layer categorizes the digit into one of the  10 possible output categories that the images can be (categories are digits 0-9 and the correct digit will have output 1, all others output 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile model----------------------------------------------------------------\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adam optimizer(alpha, beta1, beta2, epsilon) is a variation of the stochastic gradient descent algorithm where the learning rate is adjusted throughout. Adam adapts the parameter learning rates based on the average first moment and also the average of the second moments of the gradients (the uncentered variance). It calculates an exponential moving average of the gradient and the squared gradient, and the parameters beta1 and beta2 control the decay rates of these moving averages. It can be used for non-stationary objectives and problems with very noisy and/or sparse gradients.\n",
    "\n",
    "The loss function is equivalent to the cost function (J). When solving an optimization problem we are trying to minimise the loss function as this value represents the error in the output of the network. This error can be propagated back through the network, and the network weights can be adjusted accordingly. If your outputs are ones and zeros you can use categorical_crossentropy, but if they are integers 1,2,3,4... is is better to use sparse_categorical_crossentropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "55000/55000 [==============================] - 69s 1ms/step - loss: 0.0339 - acc: 0.9891\n",
      "Epoch 2/5\n",
      "55000/55000 [==============================] - 69s 1ms/step - loss: 0.0245 - acc: 0.9921\n",
      "Epoch 3/5\n",
      "55000/55000 [==============================] - 68s 1ms/step - loss: 0.0197 - acc: 0.9937\n",
      "Epoch 4/5\n",
      "55000/55000 [==============================] - 69s 1ms/step - loss: 0.0156 - acc: 0.9945\n",
      "Epoch 5/5\n",
      "55000/55000 [==============================] - 69s 1ms/step - loss: 0.0124 - acc: 0.9959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11c808455c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model--------------------------------------------------------------------\n",
    "\n",
    "model.fit(training_features , training_labels, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .fit() function trains the model on the training features and compares the outputs to the training labels. An epoch is a single pass through the whole dataset, where each training sample in the set is presented to the model once. Therefore the number of epochs is the number of complete cyles through the training data. The loss should decrease with every epoch until it levels out. The algorithm continues to run for the number of epochs you have specified, but can be stopped when the loss stops decreasing (you can set a condition for this)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 469us/step\n",
      "\n",
      "acc: 98.85%\n"
     ]
    }
   ],
   "source": [
    "#Predict the test set digits--------------------------------------------------\n",
    "\n",
    "scores = model.evaluate(test_features, test_labels)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .predict() function predicts the test labels. We can then use the model.evaluate metrics function to display the accuracy of the model.\n",
    "Accuracy is determined by comparing the predicted labels to the groundtruths.It is possible to calculate the accuracy manually using a simple for loop as shown below, where we count the number of correct predictions by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions:  14\n"
     ]
    }
   ],
   "source": [
    "#Evaulate the accuracy of the model--------------------------------------------\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in range(0, len(test_labels)):\n",
    "    \n",
    "    pred = (np.argmax(predictions[i]))\n",
    "    \n",
    "    if test_labels[i][0] == pred:\n",
    "        \n",
    "        count +=1\n",
    "\n",
    "print(\"Correct predictions: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
